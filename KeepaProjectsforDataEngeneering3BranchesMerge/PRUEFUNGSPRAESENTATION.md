# Keeper System ‚Äî Pr√ºfungspr√§sentation

> Dieses Dokument ist dein roter Faden f√ºr die m√ºndliche Pr√ºfung.
> Lies es von oben nach unten, in genau dieser Reihenfolge.

---

## 1. Die Idee (30 Sekunden)

**Ein Satz:** "Ich habe ein System gebaut, das Amazon-Keyboard-Preise automatisch √ºberwacht und die besten Deals findet."

**Das Problem:** Wer Amazon-Preise vergleichen will, muss st√§ndig manuell nachschauen. Preise √§ndern sich mehrmals t√§glich. Das gleiche Keyboard kostet in verschiedenen EU-L√§ndern unterschiedlich viel.

**Die L√∂sung:** Ein automatisiertes System das:
- Regelm√§√üig Preise bei der Keepa API abfragt
- Deals erkennt (Preis gefallen? Rabatt gro√ü genug?)
- Ergebnisse in drei Systemen speichert (je nach St√§rke)
- User benachrichtigt wenn ihr Wunschpreis erreicht ist

**Zielgruppe:** Keyboard-Enthusiasten und Schn√§ppchenj√§ger im EU-Raum.

---

## 2. Der Tech Stack (2 Minuten)

### Warum diese 6 Technologien?

| Technologie | Aufgabe | Warum genau diese? |
|---|---|---|
| **PostgreSQL** | Daten sicher speichern | Source of Truth. ACID-Garantien: entweder wird alles gespeichert oder nichts. Relationale Verkn√ºpfungen (User ‚Üí Watch ‚Üí Alert) |
| **Apache Kafka** | Events streamen | Entkoppelt Producer von Consumer. Neue Services hinzuf√ºgen ohne bestehenden Code zu √§ndern. Puffert Nachrichten wenn ein Consumer ausf√§llt |
| **Elasticsearch** | Deals durchsuchbar machen | Full-Text-Suche ("finde alle mechanischen Keyboards unter 50‚Ç¨"). Aggregationen (Durchschnittspreis, Top-Rabatte). Millisekunden-Antwortzeiten |
| **Kibana** | Dashboards & Visualisierung | Zeigt Preistrends, Deal-Statistiken, Token-Verbrauch als Grafiken. Sitzt auf Elasticsearch |
| **FastAPI** | REST-API f√ºr Zugriff von au√üen | Async, automatische API-Dokumentation unter /docs. Endpoints f√ºr Watches erstellen, Deals suchen, Health-Check |
| **Docker Compose** | Alles zusammen starten | Ein Befehl startet alle 7 Container. Reproduzierbar auf jedem Rechner |

### Warum DREI Speichersysteme?

Jedes System hat eine Superkraft die die anderen nicht haben:

```
PostgreSQL  ‚Üí Daten SICHER speichern (ACID, Transaktionen, Relationen)
Kafka       ‚Üí Events ENTKOPPELT weiterleiten (Replay, Pufferung, Consumer Groups)
Elastic     ‚Üí Daten SCHNELL durchsuchen (Full-Text, Aggregationen, Dashboards)
```

Ohne PostgreSQL: Datenverlust bei Crashes m√∂glich.
Ohne Elasticsearch: Keine effiziente Keyword-Suche √ºber tausende Deals.
Ohne Kafka: Jede neue Funktion erfordert √Ñnderungen am Scheduler.

---

## 3. Die Architektur (2 Minuten)

### Das Gesamtbild

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       KEEPER SYSTEM                          ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   ‚è∞ SCHEDULER (alle 6h Preise, alle 1h Deals)              ‚îÇ
‚îÇ       ‚îÇ                                                      ‚îÇ
‚îÇ       ‚ñº                                                      ‚îÇ
‚îÇ   üåê KEEPA API CLIENT (Token Bucket: 20 Tokens/Min)         ‚îÇ
‚îÇ       ‚îÇ                                                      ‚îÇ
‚îÇ       ‚ñº                                                      ‚îÇ
‚îÇ   üìù TRIPLE-WRITE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ       ‚îÇ                 ‚îÇ              ‚îÇ              ‚îÇ      ‚îÇ
‚îÇ       ‚ñº                 ‚ñº              ‚ñº              ‚îÇ      ‚îÇ
‚îÇ   üêò PostgreSQL    üì® Kafka      üîç Elasticsearch    ‚îÇ      ‚îÇ
‚îÇ   (Port 5432)      (Port 9092)   (Port 9200)         ‚îÇ      ‚îÇ
‚îÇ   Source of Truth   2 Topics      3 Indices           ‚îÇ      ‚îÇ
‚îÇ                     ‚îÇ                   ‚îÇ             ‚îÇ      ‚îÇ
‚îÇ                     ‚ñº                   ‚ñº             ‚îÇ      ‚îÇ
‚îÇ              2 Consumer Groups    üìä Kibana (5601)    ‚îÇ      ‚îÇ
‚îÇ                                                       ‚îÇ      ‚îÇ
‚îÇ   üîî ALERT DISPATCHER (Email / Telegram / Discord)    ‚îÇ      ‚îÇ
‚îÇ                                                       ‚îÇ      ‚îÇ
‚îÇ   üåê FASTAPI (Port 8000) ‚Äî REST-API + Swagger Docs   ‚îÇ      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Die 7 Docker Container

| Container | Was er tut | Port |
|---|---|---|
| **db** | PostgreSQL ‚Äî speichert Watches, Alerts, Deals, User | 5432 |
| **kafka** | Apache Kafka ‚Äî Event Streaming zwischen Services | 9092 |
| **zookeeper** | Verwaltet Kafka (interne Koordination) | 2181 |
| **elasticsearch** | Such-Engine f√ºr Deals (3 Indices) | 9200 |
| **kibana** | Dashboard-Visualisierung f√ºr Elasticsearch | 5601 |
| **app** | FastAPI Web-Server ‚Äî REST-API | 8000 |
| **scheduler** | Das Herzst√ºck ‚Äî zwei Loops, steuert alles | ‚Äî |

### Die 7 PostgreSQL-Tabellen

| Tabelle | Zweck |
|---|---|
| `users` | Registrierte Nutzer |
| `watched_products` | Welche ASINs werden √ºberwacht, mit Zielpreis |
| `price_alerts` | Ausgel√∂ste Benachrichtigungen |
| `price_history` | Preisverlauf √ºber Zeit |
| `collected_deals` | Gefundene Deals aus dem Deal Collector |
| `deal_filters` | User-definierte Suchfilter f√ºr Deal Reports |
| `deal_reports` | Generierte Berichte |

### Die 3 Kafka Topics

| Topic | Inhalt | Consumer Group |
|---|---|---|
| `price-updates` | Preis√§nderungen von Watches | `keeper-consumer-group` |
| `deal-updates` | Neue Deals vom Deal Collector | `keeper-consumer-group-deals` |
| `keepa-raw-deals` | Rohdaten direkt von Keepa | ‚Äî |

### Die 3 Elasticsearch Indices

| Index | Inhalt | Kibana Dashboard |
|---|---|---|
| `keeper-prices` | Preis-Updates mit Zeitstempel | Preis-Trends |
| `keeper-deals` | Gesammelte Deals (1046 Dokumente) | Deal-√úbersicht |
| `keeper-metrics` | API Token-Verbrauch pro Call | Token Budget |

---

## 4. Der Datenfluss (2 Minuten)

### Loop A: Price Watch (alle 6 Stunden)

```
1. Scheduler holt aktive Watches aus PostgreSQL
       ‚Üì
2. F√ºr jedes Watch ‚Üí Keepa API fragen (parallel, max 5 gleichzeitig)
       ‚Üì
3. Triple-Write: Preis ‚Üí PostgreSQL + Kafka + Elasticsearch
       ‚Üì
4. Wenn Preis ‚â§ Zielpreis ‚Üí Alert erstellen ‚Üí User benachrichtigen
```

### Loop B: Deal Collector (jede Stunde)

```
1. Seed-ASINs aus Datei laden (50 Keyboards)
       ‚Üì
2. Batch an Keepa API schicken ‚Üí Preise zur√ºckbekommen
       ‚Üì
3. Ist es ein Keyboard? (Title-Keywords: "tastatur", "keyboard", etc.)
       ‚Üì
4. Rabatt berechnen: (1 - aktueller_preis / listenpreis) √ó 100
       ‚Üì
5. In Elasticsearch indexieren + in PostgreSQL speichern
```

### Was passiert bei einem Keepa API Call?

```
Mein System                              Keepa Server
    ‚îÇ                                        ‚îÇ
    ‚îÇ  HTTP GET /product?asin=B09V3KXJPB     ‚îÇ
    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇ
    ‚îÇ                                        ‚îÇ
    ‚îÇ  JSON Response: Preise in Cent,        ‚îÇ
    ‚îÇ  Rating, Titel, Kategorie              ‚îÇ
    ‚îÇ  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
    ‚îÇ                                        ‚îÇ
    ‚îÇ  Tokens verbraucht: 15                 ‚îÇ
    ‚îÇ  Tokens √ºbrig: 185                     ‚îÇ
```

Keepa liefert Preise in **Cent** (3999 = 39,99‚Ç¨) und `-1` bedeutet "kein Preis verf√ºgbar".

---

## 5. Schl√ºsselkonzepte (f√ºr Prof-Fragen)

### Token Bucket ‚Äî Rate Limiting

Die Keepa API erlaubt nur 20 Tokens pro Minute. Der Token Bucket Algorithmus:
- Bucket startet mit Tokens (aufgef√ºllt nach jedem API-Call von Keepa gemeldet)
- Jeder API-Call verbraucht Tokens (~15 pro Produktabfrage)
- Wenn leer ‚Üí System wartet asynchron (async await), blockiert nicht
- `asyncio.Lock` verhindert Race Conditions bei parallelen Requests

### Graceful Degradation ‚Äî Was wenn Kafka ausf√§llt?

- Scheduler loggt eine Warnung und **l√§uft weiter**
- Preise werden weiterhin in PostgreSQL und Elasticsearch geschrieben
- Nur die Kafka-Events gehen verloren
- Wenn Kafka wieder da ist ‚Üí neue Events flie√üen normal
- **Bewusste Design-Entscheidung:** Kein Retry-Buffer f√ºr Einfachheit

### Lazy Reconnect ‚Äî `_ensure_connections()`

Docker startet alle Container gleichzeitig. Der Scheduler kann bereit sein bevor Kafka/Elasticsearch hochgefahren sind. Vor jedem Zyklus pr√ºft `_ensure_connections()` ob die Verbindungen stehen und verbindet neu falls n√∂tig. Das verhindert dass ein fehlgeschlagener Startup die Pipeline dauerhaft deaktiviert.

### asyncio.gather() mit Semaphore

Statt 50 Preise nacheinander abzufragen (langsam), fragt das System bis zu 5 gleichzeitig ab (parallel). Der Semaphore begrenzt die Gleichzeitigkeit damit die API nicht √ºberlastet wird. `return_exceptions=True` sorgt daf√ºr dass ein fehlgeschlagener Call nicht alle anderen abbricht.

---

## 6. Live-Demo Kurzversion (3 Minuten)

Falls der Prof eine Demo sehen will ‚Äî diese 5 Befehle zeigen:

```bash
# 1. Alle Container laufen
docker-compose ps

# 2. System ist gesund
curl -s http://localhost:8000/health | python3 -m json.tool

# 3. Kafka hat echte Events
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092

# 4. Elasticsearch hat 1046 Deals
curl -s 'http://localhost:9200/keeper-deals/_count?pretty'

# 5. Scheduler-Logs zeigen echte Pipeline-Aktivit√§t
docker-compose logs --tail=20 scheduler
```

**Was man sieht:** 7 laufende Container, Health-Check gr√ºn, 3 Kafka Topics, 1046 indexierte Deals, strukturierte Pipeline-Logs mit Timestamps.

---

## 7. Bewusste Entscheidungen & Limitationen

### Was wir bewusst NICHT gemacht haben

| Entscheidung | Begr√ºndung |
|---|---|
| Kein Redis-Cache | Keepa hat eigenes Rate Limiting, Cache w√§re Over-Engineering |
| Kein Kafka-Cluster | Single-Broker reicht f√ºr ~100 Nachrichten pro Zyklus |
| Kein ES-Cluster | Single-Node f√ºr Demo/Dev, Production br√§uchte 3+ Nodes |
| Kein ML-Scoring | Regelbasiertes Scoring reicht f√ºr den Use Case |
| Kein Frontend | Kibana + Swagger UI decken Visualisierung ab |
| Kein Echtzeit-System | 6h-Intervall reicht, Echtzeit w√§re 100√ó teurer an Tokens |

### Bekannte Limitationen (ehrlich sein!)

- **Nur deutscher Markt:** Multi-Market ist architektonisch vorbereitet, aber aktuell queried der Code nur domain_id=3 (DE)
- **Seed-Daten nicht perfekt:** Manche ASINs sind keine Keyboards (Keepa-Kategorie zu breit)
- **Preise teilweise 0.0:** Keepa liefert nicht f√ºr alle ASINs Preise
- **Prototype, kein Production-System:** Single-Node ES, kein SSL, keine Auth

### Was in Production anders w√§re

```
Dev/Demo (jetzt)              ‚Üí  Production
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Single-Node ES                ‚Üí  3-Node Cluster mit Replicas
1 Kafka Broker                ‚Üí  3 Broker mit Replication Factor 2
Keine Auth                    ‚Üí  OAuth2 + API Keys
6h Intervall                  ‚Üí  Dynamisch nach Token-Budget
50 Seed-ASINs                 ‚Üí  10.000+ mit automatischer Discovery
Kein Monitoring               ‚Üí  Prometheus + Grafana
```

---

## 8. Die 5 Prof-Killer-Fragen

### "Warum Kafka UND PostgreSQL?"

> "PostgreSQL speichert den State ‚Äî was ist der aktuelle Preis. Kafka streamt die Events ‚Äî was ist passiert. PostgreSQL gibt ACID-Garantien. Kafka entkoppelt Producer von Consumer, so kann ich neue Services hinzuf√ºgen ohne bestehenden Code zu √§ndern."

### "Wie funktioniert euer Rate Limiting?"

> "Token Bucket Algorithmus: 20 Tokens pro Minute. Jeder API-Call kostet Tokens. Keepa meldet nach jedem Call die verbleibenden Tokens zur√ºck. Wenn leer, wartet mein Code asynchron ‚Äî er blockiert nicht das gesamte System. Ein asyncio Lock verhindert Race Conditions bei parallelen Requests."

### "Was passiert wenn Kafka ausf√§llt?"

> "Graceful Degradation. Der Scheduler loggt eine Warnung und l√§uft weiter. Preise werden weiterhin in PostgreSQL geschrieben. Nur die Kafka-Events gehen verloren. Das ist eine bewusste Design-Entscheidung ‚Äî kein Retry-Buffer, zugunsten von Einfachheit."

### "Warum nicht alles in einer Datenbank?"

> "Jedes System hat eine St√§rke die die anderen nicht haben. PostgreSQL: ACID-Transaktionen und relationale Verkn√ºpfungen. Elasticsearch: Full-Text-Suche in Millisekunden. Kafka: Entkopplung und Replay von Events. Man k√∂nnte alles in PostgreSQL machen, aber die Suche w√§re langsam und die Kopplung eng."

### "Wie w√ºrdet ihr das System skalieren?"

> "Drei Hebel: Kafka bekommt mehr Partitions und Consumer f√ºr parallele Verarbeitung. Elasticsearch geht von Single-Node auf einen 3-Node-Cluster mit Sharding. Der Scheduler wird auf mehrere Instanzen verteilt mit Offset-Koordination. PostgreSQL w√§re der Bottleneck ‚Äî da w√ºrde man Read Replicas oder Partitionierung der Price-History-Tabelle einsetzen."

---

## 9. Zusammenfassung in einem Absatz

> "Mein System √ºberwacht Amazon-Keyboard-Preise √ºber die Keepa API. Ein Scheduler fragt alle 6 Stunden Preise ab und sammelt jede Stunde neue Deals. Jede Preis√§nderung wird dreifach geschrieben: PostgreSQL als Source of Truth, Kafka f√ºr Event-Streaming, Elasticsearch f√ºr Full-Text-Suche. Kibana visualisiert die Daten in Dashboards. Wenn ein Preis unter den Zielpreis f√§llt, wird der User per Email, Telegram oder Discord benachrichtigt. Das System l√§uft als 7 Docker Container und ist √ºber eine FastAPI REST-API erreichbar."

---

*Erstellt am 2026-02-26 ‚Äî Keeper System Pr√ºfungspr√§sentation*
